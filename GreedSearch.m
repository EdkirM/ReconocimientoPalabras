clc; clear;

% Cargar caracter铆sticas y etiquetas
load('caracteristicas.mat');  % Variables: X, y

% Codificar etiquetas
y = categorical(y);

% Divisi贸n fija de entrenamiento/prueba (80/20)
cvHoldout = cvpartition(y, 'HoldOut', 0.2);
Xtrain = X(training(cvHoldout), :);
ytrain = y(training(cvHoldout));
Xtest  = X(test(cvHoldout), :);
ytest  = y(test(cvHoldout));

%% ==== GRID SEARCH PARA SVM (RBF Kernel) ====
disp('Buscando mejores par谩metros para SVM...');

% Hiperpar谩metros a explorar
kernelScaleVals = [0.1, 1, 10];
boxConstraintVals = [0.1, 1, 10];

bestAccSVM = 0;
bestSVMModel = [];

for k = 1:length(kernelScaleVals)
    for b = 1:length(boxConstraintVals)
        % Modelo SVM con kernel RBF
        t = templateSVM('KernelFunction', 'rbf', ...
                        'KernelScale', kernelScaleVals(k), ...
                        'BoxConstraint', boxConstraintVals(b));
        modelo = fitcecoc(Xtrain, ytrain, 'Learners', t, ...
                          'KFold', 5);  % Validaci贸n cruzada
        
        % Accuracy promedio en CV
        acc = 1 - kfoldLoss(modelo);
        fprintf('SVM RBF - Scale: %.2f, Box: %.2f => Acc: %.2f%%\n', ...
                kernelScaleVals(k), boxConstraintVals(b), acc * 100);
        
        if acc > bestAccSVM
            bestAccSVM = acc;
            bestSVMModel = fitcecoc(Xtrain, ytrain, 'Learners', t);
        end
    end
end

% Evaluaci贸n final
ypredSVM = predict(bestSVMModel, Xtest);
accFinalSVM = sum(ypredSVM == ytest) / numel(ytest);
fprintf('\n Mejor SVM => Acc: %.2f%%\n', accFinalSVM * 100);

%% ==== GRID SEARCH PARA RANDOM FOREST ====
disp('Buscando mejores par谩metros para Random Forest...');

% Hiperpar谩metros a explorar
numTreesVals = [50, 100, 200];
minLeafVals = [1, 5, 10];

bestAccRF = 0;
bestRFModel = [];

for t = 1:length(numTreesVals)
    for l = 1:length(minLeafVals)
        modelo = TreeBagger(numTreesVals(t), Xtrain, ytrain, ...
                            'MinLeafSize', minLeafVals(l), ...
                            'Method', 'classification', ...
                            'OOBPrediction', 'On');

        % Evaluar usando OOB error
        oobErr = oobError(modelo, 'Mode', 'ensemble');
        acc = 1 - oobErr(end);
        fprintf('RF - Trees: %d, MinLeaf: %d => OOB Acc: %.2f%%\n', ...
                numTreesVals(t), minLeafVals(l), acc * 100);
        
        if acc > bestAccRF
            bestAccRF = acc;
            bestRFModel = modelo;
        end
    end
end

% Evaluaci贸n final
ypredRF = predict(bestRFModel, Xtest);
ypredRF = categorical(ypredRF);
accFinalRF = sum(ypredRF == ytest) / numel(ytest);
fprintf('\n Mejor RF => Acc: %.2f%%\n', accFinalRF * 100);

%% ==== Matrices de confusi贸n ====
figure;
confusionchart(ytest, ypredSVM);
title('Matriz de confusi贸n - Mejor SVM');

figure;
confusionchart(ytest, ypredRF);
title('Matriz de confusi贸n - Mejor Random Forest');
